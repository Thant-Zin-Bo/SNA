{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6c7692",
   "metadata": {},
   "source": [
    "# Phase 2: Data Preprocessing & Quality Assurance\n",
    "**Project:** Multiplex Dynamics of Polarization\n",
    "**Objective:** Prepare raw Twitter data for Master-level Network Analysis and Topic Modeling (LDA).\n",
    "\n",
    "## The Pipeline\n",
    "To ensure validity, we apply a strict 3-Layer Filter:\n",
    "1.  **Noise Filter:** Removes duplicates and short text (< 4 words) to prevent \"Garbage In, Garbage Out\".\n",
    "2.  **Bot Filter:** Removes hyper-active users (Top 0.5%) of user tweet distribution to prevent network centrality skewing (RQ1).\n",
    "3.  **NLP Normalization:** Uses **spaCy** for Lemmatization (reducing dimensionality) and Part-of-Speech filtering (RQ2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61a40d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ spaCy model 'en_core_web_sm' loaded.\n",
      "‚úÖ Preprocessing module reloaded.\n"
     ]
    }
   ],
   "source": [
    "# [Cell 2] Imports\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import importlib \n",
    "\n",
    "# Auto-install dependencies\n",
    "try:\n",
    "    import fasttext\n",
    "except ImportError:\n",
    "    !pip install fasttext-wheel\n",
    "\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "import eda\n",
    "import preprocessing as prep\n",
    "\n",
    "# FORCE RELOAD to catch your new changes\n",
    "importlib.reload(prep)\n",
    "print(\"‚úÖ Preprocessing module reloaded.\")\n",
    "\n",
    "# Config\n",
    "TRUMP_PATH = '../data/raw/hashtag_donaldtrump.csv'\n",
    "BIDEN_PATH = '../data/raw/hashtag_joebiden.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8ea39",
   "metadata": {},
   "source": [
    "Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46bb2624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Raw Data (Limit=None) ---\n",
      "üìÇ Loading data from: ../data/raw/hashtag_donaldtrump.csv...\n",
      "‚úÖ Loaded 970,919 tweets.\n",
      "üìÇ Loading data from: ../data/raw/hashtag_joebiden.csv...\n",
      "‚úÖ Loaded 776,886 tweets.\n"
     ]
    }
   ],
   "source": [
    "# [Cell 3] Load Data\n",
    "LIMIT = None # Set to None for full run\n",
    "print(f\"--- Loading Raw Data (Limit={LIMIT}) ---\")\n",
    "df_trump_raw = eda.load_data(TRUMP_PATH, limit=LIMIT)\n",
    "df_biden_raw = eda.load_data(BIDEN_PATH, limit=LIMIT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664a0c8f",
   "metadata": {},
   "source": [
    "Pipe Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92064284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_forked_pipeline(df, candidate_name):\n",
    "    if df is None: return None\n",
    "    \n",
    "    print(f\"\\nüöÄ Processing {candidate_name} Network...\")\n",
    "    \n",
    "    # 1. Language Filter (Saves Foreign to CSV)\n",
    "    if hasattr(prep, 'filter_language'):\n",
    "        df = prep.filter_language(df, save_prefix=candidate_name.lower())\n",
    "    else:\n",
    "        print(\"‚ùå Error: filter_language not found!\")\n",
    "    \n",
    "    # 2. Noise Filter\n",
    "    df = prep.filter_noise(df)\n",
    "    \n",
    "    # 3. Bot Filter (Saves Bots to CSV) <--- NEW UPDATE HERE\n",
    "    # We pass the prefix so it saves 'trump_bots_removed.csv'\n",
    "    df = prep.remove_bots(df, save_prefix=candidate_name.lower())\n",
    "    \n",
    "    # 4. LDA & BERT Prep\n",
    "    print(\"   üîπ Generating LDA Text...\")\n",
    "    df['lda_text'] = prep.spacy_clean(df['tweet'].tolist())\n",
    "    \n",
    "    print(\"   üîπ Generating BERT Text...\")\n",
    "    df['bert_text'] = prep.bert_clean(df['tweet'].tolist())\n",
    "    \n",
    "    df = df[(df['lda_text'] != \"\") & (df['bert_text'] != \"\")]\n",
    "    \n",
    "    print(f\"‚úÖ {candidate_name} Done. Count: {len(df):,}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc27f932",
   "metadata": {},
   "source": [
    "Execute Pipeline (Step-by-Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64b74b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Processing TRUMP Network...\n",
      "‚úÖ FastText model loaded.\n",
      "   üåç [Language Filter] Checking 970,919 tweets...\n",
      "      -> üíæ Saved 237,821 foreign tweets to: ../data/processed/trump_foreign_removed.csv\n",
      "      -> Retained 733,098 English tweets.\n",
      "   üßπ [Noise Filter] Starting with 733,098 tweets...\n",
      "      -> Retained 701,078 high-quality tweets.\n",
      "   ü§ñ [Bot Filter] Identifying top 0.5% active users...\n",
      "      -> Found 1,050 bot accounts (Threshold > 64 tweets).\n",
      "      -> Removed 145,398 tweets total.\n",
      "      -> üíæ Saved bot tweets to: ../data/processed/trump_bots_removed.csv\n",
      "   üîπ Generating LDA Text...\n",
      "   üß† [LDA Prep] Heavy cleaning 555,680 tweets...\n",
      "   üîπ Generating BERT Text...\n",
      "   ü§ñ [BERT Prep] Light cleaning 555,680 tweets...\n",
      "‚úÖ TRUMP Done. Count: 536,906\n",
      "\n",
      "üöÄ Processing BIDEN Network...\n",
      "   üåç [Language Filter] Checking 776,886 tweets...\n",
      "      -> üíæ Saved 177,782 foreign tweets to: ../data/processed/biden_foreign_removed.csv\n",
      "      -> Retained 599,104 English tweets.\n",
      "   üßπ [Noise Filter] Starting with 599,104 tweets...\n",
      "      -> Retained 570,200 high-quality tweets.\n",
      "   ü§ñ [Bot Filter] Identifying top 0.5% active users...\n",
      "      -> Found 1,176 bot accounts (Threshold > 37 tweets).\n",
      "      -> Removed 94,173 tweets total.\n",
      "      -> üíæ Saved bot tweets to: ../data/processed/biden_bots_removed.csv\n",
      "   üîπ Generating LDA Text...\n",
      "   üß† [LDA Prep] Heavy cleaning 476,027 tweets...\n",
      "   üîπ Generating BERT Text...\n",
      "   ü§ñ [BERT Prep] Light cleaning 476,027 tweets...\n",
      "‚úÖ BIDEN Done. Count: 457,426\n"
     ]
    }
   ],
   "source": [
    "# [Cell 5] Execute\n",
    "df_trump_final = run_forked_pipeline(df_trump_raw, \"TRUMP\")\n",
    "df_biden_final = run_forked_pipeline(df_biden_raw, \"BIDEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea5c74c",
   "metadata": {},
   "source": [
    "Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae2434bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ All files saved successfully:\n",
      "   1. *_foreign_removed.csv (Garbage)\n",
      "   2. *_lda_ready.csv (For Topic Modeling)\n",
      "   3. *_bert_ready.csv (For Sentiment/Deep Learning)\n"
     ]
    }
   ],
   "source": [
    "# [Cell 6] Save Outputs\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# 1. Save LDA Versions\n",
    "df_trump_final[['tweet', 'lda_text', 'user_id', 'created_at']].to_csv('../data/processed/trump_lda_ready.csv', index=False)\n",
    "df_biden_final[['tweet', 'lda_text', 'user_id', 'created_at']].to_csv('../data/processed/biden_lda_ready.csv', index=False)\n",
    "\n",
    "# 2. Save BERT Versions\n",
    "df_trump_final[['tweet', 'bert_text', 'user_id', 'created_at']].to_csv('../data/processed/trump_bert_ready.csv', index=False)\n",
    "df_biden_final[['tweet', 'bert_text', 'user_id', 'created_at']].to_csv('../data/processed/biden_bert_ready.csv', index=False)\n",
    "\n",
    "print(\"\\nüíæ All files saved successfully:\")\n",
    "print(\"   1. *_foreign_removed.csv (Garbage)\")\n",
    "print(\"   2. *_lda_ready.csv (For Topic Modeling)\")\n",
    "print(\"   3. *_bert_ready.csv (For Sentiment/Deep Learning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf9535d",
   "metadata": {},
   "source": [
    "bot audit check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a18402d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üïµÔ∏è‚Äç‚ôÇÔ∏è FINAL AUDIT OF REMOVED DATA ---\n",
      "\n",
      "üïµÔ∏è‚Äç‚ôÇÔ∏è BOT FILE AUDIT: trump_bots_removed.csv\n",
      "   ‚Ä¢ Total Suspicious Tweets: 145,398\n",
      "   ‚Ä¢ Total Suspicious Accounts: 1,050\n",
      "\n",
      "   üö© TOP OFFENDER (User 74268619.0):\n",
      "      - Posted 1338 times in 23.3 days.\n",
      "      - Speed: 57.5 tweets/day\n",
      "\n",
      "   ü§ñ Top Sources in this file:\n",
      "source\n",
      "Twitter Web App        62739\n",
      "Twitter for iPhone     30228\n",
      "Twitter for Android    28092\n",
      "Twitter for iPad        7751\n",
      "TweetDeck               2944\n",
      "\n",
      "   üìù Sample Content:\n",
      "['LE FIGARO \\nhttps://t.co/icqQ2j2b3u #r2p #LeFigaro #France #Macron #USA #Covid #Coronavirus #Biden #Trump #DupondMoretti #PS #LREM #Insoumis #EELV #LR #Zuckerberg #Castex #Facebook #Merkel #Beyrouth #Loukachenko #Terrorisme #BorisJohnson #Attentat #Erdogan #Nadal #RG #CouvreFeu https://t.co/uDO2Z5Tnb8'\n",
      " 'LIBERATION \\nhttps://t.co/n75plp13S6 #r2p #Liberation #UE #Europe #USA #Macron #Biden #Trump #LR #LREM #EELV #PS #CAC40 #FMI #Insoumis #Covid #Poutine #Coronavirus #Vaccin #Mbapp√© #Beyrouth #Loukachenko #Erdogan #PSG #RN #Bio #M√©lenchon #FN #Veolia #Suez #CouvreFeu https://t.co/R3ZaFiefjp'\n",
      " \"L'HUMANITE \\nhttps://t.co/e50mdPBGmm #r2p #LHumanit√© #UE #USA #France #Macron #Trump #LR #LRM #PS #Insoumis #EELV #Covid #Coronavirus #Castex #PCF #PMA #Beyrouth #Poutine #PSG #TourdeFrance #Loukachenko #Tikhanovska√Øa #Mbapp√© #MohammedVI #Fiasco #Mulliez #Bridgestone #CouvreFeu https://t.co/W6trzjkhs0\"]\n",
      "\n",
      "üïµÔ∏è‚Äç‚ôÇÔ∏è BOT FILE AUDIT: biden_bots_removed.csv\n",
      "   ‚Ä¢ Total Suspicious Tweets: 94,173\n",
      "   ‚Ä¢ Total Suspicious Accounts: 1,176\n",
      "\n",
      "   üö© TOP OFFENDER (User 1.2449822077033308e+18):\n",
      "      - Posted 1259 times in 19.0 days.\n",
      "      - Speed: 66.2 tweets/day\n",
      "\n",
      "   ü§ñ Top Sources in this file:\n",
      "source\n",
      "Twitter Web App        44649\n",
      "Twitter for iPhone     20107\n",
      "Twitter for Android    16916\n",
      "Twitter for iPad        5406\n",
      "TweetDeck               1905\n",
      "\n",
      "   üìù Sample Content:\n",
      "['Climat change is real, #wtpSenate #wtp #wtp2020 #MSNBC2020 #msnbc #JoeBiden #JOEBIDEN2020 #joebidenkamalaharris https://t.co/9m8hf51phR'\n",
      " '#DemCast #DemCastMI #wtpSenate #wtp #wtpGOTV #BidenWonTheDebate #Biden #BidenHarris #BidenHarris2020 #BLM https://t.co/evFGJguJBb'\n",
      " '#DemCast #DemCastMI #wtpSenate #wtp #wtpGOTV #BidenWonTheDebate #Biden #BidenHarris #BidenHarris2020 #BLM https://t.co/nTJP4AYZWm']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"--- üïµÔ∏è‚Äç‚ôÇÔ∏è FINAL AUDIT OF REMOVED DATA ---\")\n",
    "\n",
    "# Audit Trump Bots\n",
    "prep.analyze_bot_file('../data/processed/trump_bots_removed.csv')\n",
    "\n",
    "# Audit Biden Bots\n",
    "prep.analyze_bot_file('../data/processed/biden_bots_removed.csv')\n",
    "\n",
    "# Audit Trump Foreign (Optional Check)\n",
    "# prep.analyze_bot_file('../data/processed/trump_foreign_removed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
